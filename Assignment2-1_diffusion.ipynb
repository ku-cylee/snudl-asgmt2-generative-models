{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eHzSrONS0-YlRLBxqmbMqTyUR6GvxfTd","timestamp":1698821426650},{"file_id":"1E40GbiEfkIIw_VOFjTxn-1eFVe2kpQ9j","timestamp":1698305294075}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# M2177.003100 Deep Learning Assignment #2<br> Part 1. Denoising Diffusion Probabilistic Models (Pytorch)\n","\n","---"],"metadata":{"id":"_toP4pObJqmh"}},{"cell_type":"markdown","source":["\n","\n","Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them.\n","\n","**For understanding of this work, please carefully look at given PDF file.**\n","\n","In this notebook, you will learn how to train denoising diffusion probabilistic models.\n","There are 3 sections, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n","\n","**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n","\n","**DO NOT clear the final outputs so that TAs can grade both your code and results.**"],"metadata":{"id":"OyLTeRgQJyZU"}},{"cell_type":"markdown","metadata":{"id":"QYOlvQ1nQL7c"},"source":["## 1. Setup\n","\n","First, please make sure you are using a GPU runtime to run this notebook, so inference is much faster."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFLZsBSGJEPq"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"paJt_cx5QgVz"},"source":["Next, install and import necessary libraries.\n","You can import additional libraries if necessary,<br>\n","but using libraries such as diffusers which already have the materials for this assignment are **NOT** allowed."]},{"cell_type":"code","source":["!pip3 install torch torchvision"],"metadata":{"id":"XfiWEjgqJO9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from typing import Dict, Tuple\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import models, transforms\n","from torchvision.datasets import MNIST\n","from torchvision.utils import save_image, make_grid\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation, PillowWriter\n","import numpy as np"],"metadata":{"id":"zie4BIg8Jhxu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download and setup the CIFAR-10 dataset we will be using."],"metadata":{"id":"ORm4ZEqmMOX1"}},{"cell_type":"code","source":["###### Data Setting\n","\n","batch_size = 256                 # the number of data in each iteration\n","\n","# optionally load a model\n","tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n","\n","dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"],"metadata":{"id":"MIs6TlGf2Kcm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check example images."],"metadata":{"id":"bImRcHnUMcZz"}},{"cell_type":"code","source":["sample_batch = next(iter(dataloader))[0][:100]\n","\n","plt.imshow(make_grid(sample_batch, nrow=10).permute(1, 2, 0))\n","plt.show()\n","plt.close()"],"metadata":{"id":"tQOMgQ4V2PVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Noise Scheduling\n","### 2.1 Beta schedule: implement the two typically used noise schedule for diffusion models:\n","* Linear schedule<br>\n",": linearly increase $\\beta_t$ from $\\beta_1=10^{-4}$ to $\\beta_T = 0.02$\n","\n","* Cosine schedule<br>\n",": $\\beta_t$ = clip$(1 - \\frac{\\bar{\\alpha_t}}{\\bar{\\alpha}_{t-1}}, 0.999)$,<br>\n","$\\bar{\\alpha_t} = \\frac{f(t)}{f(0)}$ where $f(t) = \\cos^2(\\frac{t/T + s}{1 + s} \\cdot \\frac{\\pi}{2})$ <br>\n","small offset $s$: to prevent $\\beta_t$ from being too small when close to 0. Here, we set $s=0.008$.\n","\n","\n","#### 2.2 Pre-compute necessary values of the noise schedule:\n","* beta_t: $\\beta_t$\n","* sqrt_beta_t: $\\sqrt{\\beta_t}$\n","* alpha_t: $\\alpha_t:=1-\\beta_t$\n","* alphabar_t: $\\bar{\\alpha_t} := ‚àè_{i=1}^{t}\\alpha_t$\n","* sqrtab: $\\sqrt{\\bar{\\alpha_t}}$\n","* oneover_sqrta: $\\frac{1}{\\sqrt{\\alpha_t}}$\n","* sqrtmab: $\\sqrt{1-\\bar{\\alpha_t}}$\n","* bt_over_sqrtmab: $\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha_t}}}$\n","\n","\n"],"metadata":{"id":"Ztwtx9ZAMnGn"}},{"cell_type":"code","source":["def beta_schedule(beta1, beta2, T, schedule='linear'):\n","    ##############################################################################\n","    #                          IMPLEMENT YOUR CODE                               #\n","    ##############################################################################\n","\n","    if schedule == 'linear':\n","        betas = None            # TODO\n","\n","\n","    elif schedule == 'cosine':\n","        betas = None            # TODO\n","        s = 0.008\n","\n","\n","    ##############################################################################\n","    #                          END OF YOUR CODE                                  #\n","    ##############################################################################\n","    return betas\n","\n","def ddpm_schedules(beta1, beta2, T, schedule='linear'):\n","    \"\"\"\n","    Returns pre-computed schedules for DDPM sampling, training process.\n","    \"\"\"\n","    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n","\n","\n","    beta_t = beta_schedule(beta1, beta2, T, schedule)\n","\n","    ##############################################################################\n","    #                          IMPLEMENT YOUR CODE                               #\n","    ##############################################################################\n","\n","    sqrt_beta_t = None           # TODO\n","    alpha_t = None               # TODO\n","    alphabar_t = None            # TODO\n","\n","    sqrtab = None                # TODO\n","    oneover_sqrta = None         # TODO\n","\n","    sqrtmab = None               # TODO\n","    bt_over_sqrtmab_inv = None   # TODO\n","\n","    ##############################################################################\n","    #                          END OF YOUR CODE                                  #\n","    ##############################################################################\n","\n","    return {\n","        \"beta_t\": beta_t,                          # \\beta_t\n","        \"sqrt_beta_t\": sqrt_beta_t,                # \\sqrt{\\beta_t}\n","        \"alpha_t\": alpha_t,                        # \\alpha_t\n","        \"alphabar_t\": alphabar_t,                  # \\bar{\\alpha_t}\n","        \"sqrtab\": sqrtab,                          # \\sqrt{\\bar{\\alpha_t}}\n","        \"oneover_sqrta\": oneover_sqrta,            # 1/\\sqrt{\\alpha_t}\n","        \"sqrtmab\": sqrtmab,                        # \\sqrt{1-\\bar{\\alpha_t}}\n","        \"bt_over_sqrtmab\": bt_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n","    }\n","\n"],"metadata":{"id":"otEADZ-8KkcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize and check your implementation of scheduling."],"metadata":{"id":"SIS_wrymNYfN"}},{"cell_type":"code","source":["# ddpm scheduling check\n","\n","import matplotlib.pyplot as plt\n","\n","n_T = 1000\n","\n","ddpm_scheduling_dict = ddpm_schedules(1e-4, 0.02, n_T, schedule='linear')\n","\n","beta = ddpm_scheduling_dict['beta_t']\n","alpha = ddpm_scheduling_dict['alpha_t']\n","alpha_bar = ddpm_scheduling_dict['alphabar_t']\n","\n","fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,6))\n","\n","axes[0].plot(np.arange(len(alpha)), alpha)\n","axes[0].set_xlabel('timesteps')\n","axes[0].set_ylabel('alpha')\n","\n","axes[1].plot(np.arange(len(alpha_bar)), alpha_bar)\n","axes[1].set_xlabel('timesteps')\n","axes[1].set_ylabel('alpha_bar')\n","\n","axes[2].plot(np.arange(len(alpha_bar)), np.log10(alpha_bar / (1 - alpha_bar)))\n","axes[2].set_xlabel('timesteps')\n","axes[2].set_ylabel('log-SNR')\n","\n","plt.show()\n","plt.close()"],"metadata":{"id":"Gk_5rm4poQee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize and check the forward diffusion process with your implemented schedule.\n","\n","* **forward_list** will visualize the original sequential forward process\n","* **forward_list2** will visualize the forward process using reparameterization trick.\n","\n","If implemented correctly, the two should look alike."],"metadata":{"id":"KSNhVyBdPTH9"}},{"cell_type":"code","source":["sample_batch = next(iter(dataloader))[0][:10]\n","\n","forward_list = []\n","forward_list2 = []\n","\n","x = sample_batch\n","for t in range(n_T):\n","    x = torch.sqrt(1 - ddpm_scheduling_dict['beta_t'][t, None, None, None]) * x + torch.sqrt(ddpm_scheduling_dict['beta_t'][t, None, None, None]) * torch.randn_like(x)\n","    if t % (n_T//10) == 0:\n","        forward_list.append(x.detach().cpu())\n","\n","for t in range(n_T):\n","    if t % (n_T//10) == 0:\n","        x = ddpm_scheduling_dict['sqrtab'][t, None, None, None] * sample_batch + ddpm_scheduling_dict['sqrtmab'][t, None, None, None] * torch.randn_like(sample_batch)\n","        forward_list2.append(x.detach().cpu())\n","\n","plt.imshow(make_grid(torch.cat(forward_list), nrow=10).permute(1, 2, 0), cmap='gray')\n","plt.show()\n","plt.close()\n","\n","plt.imshow(make_grid(torch.cat(forward_list2), nrow=10).permute(1, 2, 0), cmap='gray')\n","plt.show()\n","plt.close()"],"metadata":{"id":"h9k8xnx-szY9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. The U-Net architecture for diffusion models.\n","Diffusion Models typically use a timestep-conditioned U-Net architecture. <br>\n","In this assignment, the U-Net is given as below. <br>\n","You may adjust the architecture if necessary."],"metadata":{"id":"8JrvOSXlPdXr"}},{"cell_type":"code","source":["class ResidualConvBlock(nn.Module):\n","    def __init__(\n","        self, in_channels: int, out_channels: int, is_res: bool = False\n","    ) -> None:\n","        super().__init__()\n","        '''\n","        standard ResNet style convolutional block\n","        '''\n","        self.same_channels = in_channels==out_channels\n","        self.is_res = is_res\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.GELU(),\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.GELU(),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        if self.is_res:\n","            x1 = self.conv1(x)\n","            x2 = self.conv2(x1)\n","            # this adds on correct residual in case channels have increased\n","            if self.same_channels:\n","                out = x + x2\n","            else:\n","                out = x1 + x2\n","            return out / 1.414\n","        else:\n","            x1 = self.conv1(x)\n","            x2 = self.conv2(x1)\n","            return x2\n","\n","\n","class UnetDown(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UnetDown, self).__init__()\n","        '''\n","        process and downscale the image feature maps\n","        '''\n","        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","class UnetUp(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UnetUp, self).__init__()\n","        '''\n","        process and upscale the image feature maps\n","        '''\n","        layers = [\n","            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n","            ResidualConvBlock(out_channels, out_channels),\n","            ResidualConvBlock(out_channels, out_channels),\n","        ]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x, skip):\n","        x = torch.cat((x, skip), 1)\n","        x = self.model(x)\n","        return x\n","\n","\n","class EmbedFC(nn.Module):\n","    def __init__(self, input_dim, emb_dim):\n","        super(EmbedFC, self).__init__()\n","        '''\n","        generic one layer FC NN for embedding things\n","        '''\n","        self.input_dim = input_dim\n","        layers = [\n","            nn.Linear(input_dim, emb_dim),\n","            nn.GELU(),\n","            nn.Linear(emb_dim, emb_dim),\n","        ]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.input_dim)\n","        return self.model(x)\n","\n","\n","class ContextUnet(nn.Module):\n","    def __init__(self, in_channels, n_feat = 256, n_classes=10):\n","        super(ContextUnet, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.n_feat = n_feat\n","        self.n_classes = n_classes\n","\n","        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n","\n","        self.down1 = UnetDown(n_feat, n_feat)\n","        self.down2 = UnetDown(n_feat, 2 * n_feat)\n","\n","        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n","\n","        self.timeembed1 = EmbedFC(1, 2*n_feat)\n","        self.timeembed2 = EmbedFC(1, 1*n_feat)\n","        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n","        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n","\n","        self.up0 = nn.Sequential(\n","            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7), # otherwise just have 2*n_feat\n","            nn.GroupNorm(8, 2 * n_feat),\n","            nn.ReLU(),\n","        )\n","\n","        self.up1 = UnetUp(4 * n_feat, n_feat)\n","        self.up2 = UnetUp(2 * n_feat, n_feat)\n","        self.out = nn.Sequential(\n","            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n","            nn.GroupNorm(8, n_feat),\n","            nn.ReLU(),\n","            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n","        )\n","\n","    def forward(self, x, c, t):\n","        # x is (noisy) image, c is context label, t is timestep,\n","        # context_mask says which samples to block the context on\n","\n","        x = self.init_conv(x)\n","        down1 = self.down1(x)\n","        down2 = self.down2(down1)\n","        hiddenvec = self.to_vec(down2)\n","\n","        # convert context to one hot embedding\n","        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n","\n","        # embed context, time step\n","        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n","        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n","        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n","        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n","\n","        up1 = self.up0(hiddenvec)\n","        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n","        up3 = self.up2(cemb2*up2+ temb2, down1)\n","        out = self.out(torch.cat((up3, x), 1))\n","        return out"],"metadata":{"id":"iQAZy3k0KZpW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Build the diffusion model wrapper.\n","\n","#### 4.1 forward( ): implement the loss computation for diffusion model.\n","* Given a sample $x_0$, random timestep $t$ and noise $\\epsilon$, compute the noisy sample\n","$x_t=\\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1 - \\bar{\\alpha_t}}\\epsilon$ .\n","\n","* Forward the inputs through the network, and compute the loss between prediction and the ground-truth.\n","\n","\n","\n","#### 4.2 sample( ): Implement a sampling step with diffusion model.\n","* Given a batch of random noise **x_i** and class labels **c_i**, generate a class-conditioned image.\n","\n","* Save the intermediate results in the list **x_i_store** for visualization.\n","\n","  * Saving the intermediate results as all timesteps would be inefficient; thus it is recommended to save results every N steps (e.g., save every 25 steps)\n","\n","\n","\n","#### 4.3 Implementation for different prediction types\n","* Typically, prediction of the noise $\\epsilon$ or the original sample $x_0$ are popular choices.\n","* Implement the loss computation and the sampling process for both ($\\epsilon$, $x_0$) prediction types."],"metadata":{"id":"ukybLFn_PlJJ"}},{"cell_type":"code","source":["class DDPM(nn.Module):\n","    def __init__(self, nn_model, betas, n_T, prediction_type, schedule, device):\n","        super(DDPM, self).__init__()\n","        self.nn_model = nn_model.to(device)\n","\n","        # register_buffer allows accessing dictionary produced by ddpm_schedules\n","        # e.g. can access self.sqrtab later\n","        for k, v in ddpm_schedules(betas[0], betas[1], n_T, schedule).items():\n","            self.register_buffer(k, v)\n","\n","        self.n_T = n_T\n","        assert prediction_type in ['epsilon', 'sample'], 'prediction type needs to be either one of epsilon or sample (x_0).'\n","        self.prediction_type = prediction_type\n","        self.device = device\n","\n","    def forward(self, x, c):\n","        \"\"\"\n","        this method is used in training, sample t and noise randomly.\n","        \"\"\"\n","\n","        ##############################################################################\n","        #                          IMPLEMENT YOUR CODE                               #\n","        ##############################################################################\n","\n","        if self.prediction_type == 'epsilon':\n","            pass\n","\n","        elif self.prediction_type == 'sample':\n","            pass\n","\n","        ##############################################################################\n","        #                          END OF YOUR CODE                                  #\n","        ##############################################################################\n","\n","        return loss  # return the loss\n","\n","    def sample(self, n_sample, size, device):\n","        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n","        c_i = torch.arange(0, 10).to(device)  # the mnist labels\n","        assert n_sample % c_i.shape[0] == 0, 'number of samples must be a multiple of 10.'\n","        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))  # make n_samples\n","        x_i_store = [] # keep track of generated steps to plot the reverse process: save intermediate results\n","\n","        ##############################################################################\n","        #                          IMPLEMENT YOUR CODE                               #\n","        ##############################################################################\n","\n","\n","\n","        ##############################################################################\n","        #                          END OF YOUR CODE                                  #\n","        ##############################################################################\n","\n","        x_i_store = np.array(x_i_store)\n","        return x_i, x_i_store"],"metadata":{"id":"zldse5EOL8K9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Train\n","Now, let's try training the diffusion model.\n","Configure the hyperparameters as you like, and try to get a good generation quality.\n","\n","**DO NOT** change the two settings below.\n","* n_classes: number of classes (10) will be used for generating all ten digits.\n","* save_dir: path to save the results of your experiments. If you change it, it may be left during compression.\n","\n","You may adjust other parts of the code if necessary.\n"],"metadata":{"id":"6doU7X2xPqWW"}},{"cell_type":"code","source":["device='cuda'\n","n_epoch = None              # total training epoch\n","n_T = None                  # total timesteps of diffusion process\n","betas = None                # beta schedule\n","n_classes = 10              # the number of class in dataset\n","n_feat = None               # feature dimension of UNet\n","lr = None                   # learning rate\n","prediction_type = None      # prediction type of diffusion models: epsilon (noise) or sample (x_0)\n","schedule = None             # noise schedule\n","\n","# to save results\n","save_dir = f'./results_{prediction_type}_{schedule}/'  # DO NOT change this part, or else it could be left out when compressing.\n","os.makedirs(save_dir, exist_ok=True)\n","save_every = 5\n","\n","nn_model = ContextUnet(in_channels=1, n_feat=n_feat, n_classes=n_classes)\n","ddpm = DDPM(nn_model=nn_model, betas=betas, n_T=n_T, prediction_type=prediction_type, schedule=schedule, device=device)\n","ddpm.to(device)\n","\n","optim = None                # set optimizer for training."],"metadata":{"id":"FZBCKV9aMuDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training\n","for ep in range(n_epoch):\n","    print(f'epoch {ep}')\n","    ddpm.train()\n","\n","    pbar = tqdm(dataloader)\n","    loss_ema = None\n","    for x, c in pbar:\n","        optim.zero_grad()\n","        x = x.to(device)\n","        c = c.to(device)\n","        loss = ddpm(x, c)\n","        loss.backward()\n","        if loss_ema is None:\n","            loss_ema = loss.item()\n","        else:\n","            loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n","        pbar.set_description(f\"loss: {loss_ema:.4f}\")\n","        optim.step()\n","\n","    # for eval, save an image of currently generated samples (top rows)\n","    # followed by real images (bottom rows)\n","    if ep % save_every == 0 or ep == int(n_epoch - 1):\n","        ddpm.eval()\n","        with torch.no_grad():\n","            n_sample = 4*n_classes\n","            x_gen, x_gen_store = ddpm.sample(n_sample, (1, 28, 28), device)\n","\n","            # append some real images at bottom, order by class also\n","            x_real = torch.Tensor(x_gen.shape).to(device)\n","            for k in range(n_classes):\n","                for j in range(int(n_sample/n_classes)):\n","                    try:\n","                        idx = torch.squeeze((c == k).nonzero())[j]\n","                    except:\n","                        idx = 0\n","                    x_real[k+(j*n_classes)] = x[idx]\n","\n","            x_all = torch.cat([x_gen, x_real])\n","            grid = make_grid(x_all, nrow=10)\n","            if ep == int(n_epoch - 1):\n","                save_image(grid, save_dir + f\"image_final.png\")\n","                print('saved image at ' + save_dir + f\"image_final.png\")\n","            else:\n","                save_image(grid, save_dir + f\"image_ep{ep}.png\")\n","                print('saved image at ' + save_dir + f\"image_ep{ep}.png\")\n","\n","            # create gif of images evolving over time, based on x_gen_store\n","            fig, axs = plt.subplots(nrows=int(n_sample/n_classes), ncols=n_classes,sharex=True,sharey=True,figsize=(8,3))\n","            def animate_diff(i, x_gen_store):\n","                print(f'gif animating frame {i} of {x_gen_store.shape[0]}', end='\\r')\n","                plots = []\n","                for row in range(int(n_sample/n_classes)):\n","                    for col in range(n_classes):\n","                        axs[row, col].clear()\n","                        axs[row, col].set_xticks([])\n","                        axs[row, col].set_yticks([])\n","                        plots.append(axs[row, col].imshow(x_gen_store[i,(row*n_classes)+col,0],cmap='gray',vmin=(x_gen_store[i]).min(), vmax=(x_gen_store[i]).max()))\n","                return plots\n","            ani = FuncAnimation(fig, animate_diff, fargs=[x_gen_store],  interval=200, blit=False, repeat=True, frames=x_gen_store.shape[0])\n","            if ep == int(n_epoch - 1):\n","                ani.save(save_dir + f\"gif_final.gif\", dpi=100, writer=PillowWriter(fps=5))\n","                print('saved image at ' + save_dir + f\"gif_final.gif\")\n","            else:\n","                ani.save(save_dir + f\"gif_ep{ep}.gif\", dpi=100, writer=PillowWriter(fps=5))\n","                print('saved image at ' + save_dir + f\"gif_ep{ep}.gif\")\n","            plt.close('all')\n","\n","    # save model\n","    if ep == int(n_epoch-1):\n","        torch.save(ddpm.state_dict(), save_dir + f\"model_final.pth\")\n","        print('saved model at ' + save_dir + f\"model_final.pth\")"],"metadata":{"id":"TWu8xqcRKoD_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Inference\n","With your trained model, try generating random samples with your model."],"metadata":{"id":"m7lfZ5TJPuFF"}},{"cell_type":"code","source":["num_samples = 100\n","x_gen, x_gen_store = ddpm.sample(num_samples, (1, 28, 28), device)\n","plt.imshow(make_grid(x_gen.cpu(), nrow=10).permute(1, 2, 0))\n","plt.show()\n","plt.close()"],"metadata":{"id":"OiklP0267scf"},"execution_count":null,"outputs":[]}]}